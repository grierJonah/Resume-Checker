{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4253353-da3f-4785-9d76-74097074a21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import ollama\n",
    "from bs4 import BeautifulSoup\n",
    "from pypdf import PdfReader\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e542991-5744-4a12-8e6e-f06ae1cdb752",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### ChatGPT Credentials ###\n",
    "###########################\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai = OpenAI()\n",
    "\n",
    "################################\n",
    "### Ollama Local Credentials ###\n",
    "################################\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "OLLAMA_HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "OLLAMA_MODEL = \"llama3.2\" #'tinyllama' #'phi3:mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67b1f4d-1910-412b-8847-1ea609c3d474",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Class to store data about the uploaded Website\n",
    "class Website:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        print('Scanning URL: ', url)\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No Title Found\"\n",
    "        for irrelevant in soup.body(\"script\", \"style\", \"img\", \"input\"):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "        print('Successfully parsed Website')\n",
    "\n",
    "# Class to store data about the uploaded Resume\n",
    "class Resume:\n",
    "    def __init__(self, resume_path):\n",
    "        self.pdfReader = PdfReader(resume_path)\n",
    "        self.text = self.pdfReader.pages[0].extract_text()\n",
    "        print('Extracted Text from PDF!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5379306-7086-4ce6-87a0-5014436482f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a resume checker and have internal knowledge about what the ATS (applicant tracking system) is identifying as needed when observing the uploaded resume\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1488f979-c83d-4fce-927b-e50fe438671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inform LLM what we want to happen to the Resume we're uploading.\n",
    "###   - In this we explain that we are applying to the job listed on this Website & would like\n",
    "###     the LLM to extract the title, text and resume data to give us feedback.\n",
    "def user_prompt(website, resume):\n",
    "    user_prompt = f\"You are looking at a website titled: {website.title}, with this info: {website.text}\"\n",
    "    user_prompt += \"\\nPlease use this website to make key adjustments to the uploaded resume on hand. \\\n",
    "    I am the person looking to be hired by this company and would like to be given at least a fair chance to interview\\\n",
    "    Please suggest necessary changes and improvements in list format -- especially within the Projects or Experience sections.\\\n",
    "    Also, please make your suggestions using Markdown.\\n\\n\" \n",
    "    user_prompt += resume.text\n",
    "    return user_prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb2f8f6-10bd-42b3-ab65-3cacd2c84fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create boilerplate array of dictionaries for LLM\n",
    "def messages_for_user(website, resume):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt(website, resume)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9030c5e-eb14-45b7-820d-3f4ca8566668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_gpt(url):\n",
    "    website = Website(url)\n",
    "    resume = Resume('grier_resume.pdf')\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages_for_user(website, resume)\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544b6f1a-f065-418f-acf2-7c00c0672181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment correct LLM version, to ensure Jupyter has manifest ready and available prior to next part\n",
    "!ollama pull llama3.2\n",
    "# !ollama pull phi3:mini\n",
    "# !ollama pull tinyllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba5e634-bc29-484d-a605-6831c08945a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"model\": OLLAMA_MODEL,\n",
    "    \"headers\": OLLAMA_HEADERS,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "def summarize_ollama(url):\n",
    "    print(f'Be patient. Queing up {OLLAMA_MODEL}')\n",
    "    website = Website(url)\n",
    "    resume = Resume('grier_resume.pdf')\n",
    "    ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "    response = ollama_via_openai.chat.completions.create(\n",
    "        model=OLLAMA_MODEL,\n",
    "        messages=messages_for_user(website, resume)\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b7dd19-946d-411f-8036-260f5ff35bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_summary(url, gpt=False):\n",
    "    if gpt:\n",
    "        summary = summarize_gpt(url)\n",
    "        display(Markdown(summary))\n",
    "    else:\n",
    "        # be patient\n",
    "        print(summarize_ollama(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32eae4d-1222-407a-9cbe-3f66f2e69b02",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_summary('https://search-careers.gm.com/en/jobs/jr-202500570/data-scientist', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121450ca-1c6d-4ba1-8050-7f87d61a9d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
